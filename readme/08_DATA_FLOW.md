# ğŸ”„ Data Flow & Workflows

Understanding how data moves through DataBrew Lab from input to output.

## ğŸ“‹ Table of Contents
- [Document Upload Flow](#document-upload-flow)
- [Search Workflow](#search-workflow)
- [Live Transcription Flow](#live-transcription-flow)
- [AI Chat Workflow](#ai-chat-workflow)
- [Meeting Bot Workflow](#meeting-bot-workflow)
- [Podcast Processing Flow](#podcast-processing-flow)

## ğŸ“¤ Document Upload Flow

### Step-by-Step Process

```
1. User selects file
   â†“
2. Frontend validates file
   - Check file type (PDF, DOCX, TXT, audio)
   - Check file size (< 50MB)
   - Show upload progress
   â†“
3. Upload to backend
   POST /api/v1/data-ingestion/upload
   - multipart/form-data
   - file + metadata
   â†“
4. Backend processes upload
   - Save file to storage
   - Extract text content
   - Generate metadata
   â†“
5. Text chunking
   - Split into 500-1000 token chunks
   - Add overlap (50-100 tokens)
   - Preserve context
   â†“
6. Generate embeddings
   - Call AI service (Gemini/OpenAI)
   - Get 768-dimensional vectors
   - One embedding per chunk
   â†“
7. Store in database
   - Document record in `documents`
   - Chunks in `document_chunks`
   - Embeddings for semantic search
   â†“
8. Update status â†’ completed
   â†“
9. Frontend refreshes
   - Document appears in list
   - Ready for search
```

### Technical Implementation

```typescript
// Frontend - DataIngestion component
const handleUpload = async (file: File) => {
  const formData = new FormData();
  formData.append('file', file);
  formData.append('title', title);
  
  const response = await dataIngestionAPI.uploadFile(file, { title });
  console.log('Uploaded:', response.documentId);
};

// Backend - dataIngestion route
router.post('/upload', upload.single('file'), async (req, res) => {
  const { file } = req;
  const userId = req.user.userId;
  
  // Extract text
  const text = await extractText(file.path);
  
  // Process document
  const document = await documentProcessor.process({
    userId,
    filePath: file.path,
    filename: file.originalname,
    text
  });
  
  res.json({ success: true, data: { documentId: document.id } });
});

// Document processor service
async process(data) {
  // 1. Save document record
  const doc = await saveDocument(data);
  
  // 2. Chunk text
  const chunks = await chunkingService.chunk(data.text);
  
  // 3. Generate embeddings
  for (const chunk of chunks) {
    const embedding = await aiService.generateEmbedding(chunk.content);
    await saveChunk(doc.id, chunk, embedding);
  }
  
  // 4. Update status
  await updateDocumentStatus(doc.id, 'completed');
  
  return doc;
}
```

## ğŸ” Search Workflow

### Hybrid Search Process

```
1. User enters query
   "customer satisfaction issues"
   â†“
2. Frontend sends search request
   POST /api/v1/search/query
   { query, searchType: 'hybrid' }
   â†“
3. Backend processes query
   â”œâ”€ Path A: Keyword Search (BM25)
   â”‚  - Tokenize query
   â”‚  - Match against document text
   â”‚  - Score by term frequency
   â”‚  - Return top 20 results
   â”‚
   â””â”€ Path B: Semantic Search (Vector)
      - Generate query embedding
      - Calculate cosine similarity
      - Find nearest vectors
      - Return top 20 results
   â†“
4. Fusion (RRF - Reciprocal Rank Fusion)
   - Combine both result sets
   - Calculate fused scores
   - Re-rank combined results
   â†“
5. Add context & citations
   - Extract relevant snippets
   - Add document metadata
   - Include relevance scores
   â†“
6. Return to frontend
   { results, totalResults, searchTime }
   â†“
7. Display results
   - Highlighted matches
   - Document previews
   - Relevance indicators
```

### Search Types Comparison

| Type | Method | Best For | Speed |
|------|--------|----------|-------|
| **Keyword** | BM25 | Exact terms, names | Fast |
| **Semantic** | Vector similarity | Concepts, meaning | Medium |
| **Hybrid** | BM25 + Vector + RRF | General search | Medium |

### Implementation

```typescript
// Hybrid search service
async search(query: string, options) {
  // 1. Keyword search
  const keywordResults = await this.keywordSearch(query);
  
  // 2. Semantic search
  const embedding = await aiService.generateEmbedding(query);
  const semanticResults = await this.vectorSearch(embedding);
  
  // 3. RRF fusion
  const fusedResults = this.fuseResults(keywordResults, semanticResults);
  
  // 4. Add context
  return this.addContext(fusedResults);
}
```

## ğŸ™ï¸ Live Transcription Flow

### Real-Time Audio Processing

```
1. User clicks "Start Recording"
   â†“
2. Browser requests microphone access
   navigator.mediaDevices.getUserMedia({ audio: true })
   â†“
3. Create MediaRecorder
   - Sample rate: 16kHz
   - Format: webm/opus
   - Chunk interval: 1 second
   â†“
4. Start recording
   recorder.start(1000)
   â†“
5. Audio chunks generated
   Every 1 second:
   â”œâ”€ ondataavailable event fires
   â”œâ”€ Get audio blob
   â””â”€ Send to backend via WebSocket
      socket.emit('audio_chunk', { audio: blob })
   â†“
6. Backend receives chunk
   â”œâ”€ Buffer audio data
   â”œâ”€ When enough data collected:
   â””â”€ Send to Whisper API
      POST https://api.openai.com/v1/audio/transcriptions
   â†“
7. Whisper returns transcript
   {
     text: "This is what was said",
     language: "en",
     confidence: 0.95
   }
   â†“
8. Backend processes transcript
   â”œâ”€ Add timestamp
   â”œâ”€ Identify speaker (if diarization on)
   â””â”€ Emit to client
      socket.emit('transcript_update', { text, speaker, timestamp })
   â†“
9. Frontend displays transcript
   - Append to transcript view
   - Highlight speaker
   - Auto-scroll to latest
   â†“
10. User clicks "Stop"
    â”œâ”€ Stop recorder
    â”œâ”€ Send final chunk
    â””â”€ Save session to database
       POST /api/v1/thought-nuggets/save
```

### WebSocket Events

```typescript
// Client events
socket.emit('start_recording', { sessionId });
socket.emit('audio_chunk', { sessionId, audio });
socket.emit('stop_recording', { sessionId });

// Server events
socket.on('transcript_update', ({ text, speaker, timestamp }));
socket.on('transcription_complete', ({ sessionId, transcript }));
socket.on('transcription_error', ({ error }));
```

## ğŸ§  AI Chat Workflow

### Brain Discussion Flow

```
1. User types message
   "What are the key findings about customer satisfaction?"
   â†“
2. Frontend sends message
   POST /api/v1/brain/messages
   { content, mode: 'professional', projectId }
   â†“
3. Backend receives message
   â”œâ”€ Save to brain_messages table
   â””â”€ Process with brain service
   â†“
4. Search for relevant context
   - Generate query embedding
   - Search document chunks
   - Find top 5 relevant chunks
   â†“
5. Build AI prompt
   System: "You are a research assistant..."
   Context: [relevant document chunks]
   User: "What are the key findings..."
   â†“
6. Call AI service (Gemini/OpenAI)
   - Send prompt with context
   - Receive AI response
   - Extract citations
   â†“
7. Process AI response
   â”œâ”€ Parse markdown
   â”œâ”€ Extract mentioned documents
   â”œâ”€ Add citation links
   â””â”€ Calculate relevance scores
   â†“
8. Save response
   UPDATE brain_messages
   SET response = ?, citations = ?
   WHERE id = ?
   â†“
9. Return to frontend
   {
     response: "Based on the analysis...",
     citations: [{ documentId, snippet, relevance }],
     timestamp
   }
   â†“
10. Display in chat
    - Render markdown
    - Show citations
    - Add to history
```

## ğŸ¤– Meeting Bot Workflow

### Bot Lifecycle

```
1. User creates bot
   POST /api/v1/meetstream/bot
   { meetingUrl, botName, instructions }
   â†“
2. Backend creates bot
   â”œâ”€ Generate bot ID
   â”œâ”€ Store in database
   â””â”€ Send to meeting service
   â†“
3. Bot joins meeting
   - Connect to meeting platform
   - Authenticate
   - Join as participant
   â”œâ”€ Status â†’ joining
   â””â”€ Emit bot_created event
   â†“
4. Bot becomes active
   - Listen to audio
   - Transcribe in real-time
   - Status â†’ active
   â†“
5. User sends command
   socket.emit('agent_message', { message: "Summarize discussion" })
   â†“
6. Bot processes command
   â”œâ”€ Get meeting context (transcripts)
   â”œâ”€ Call AI with context + command
   â””â”€ Generate response
   â†“
7. Bot responds
   socket.emit('bot_message', {
     botId,
     message: "The main points discussed are..."
   })
   â†“
8. User sees response in UI
   â†“
9. Meeting ends
   - Bot leaves meeting
   - Generate summary
   - Save transcript
   - Status â†’ ended
```

## ğŸ“» Podcast Processing Flow

### End-to-End Process

```
1. User adds podcast
   POST /api/v1/podcast-transcription/add-podcast
   { podcastUrl, episodeLimit: 10 }
   â†“
2. Fetch podcast feed
   - Parse RSS/JSON feed
   - Extract metadata
   - List episodes
   â†“
3. Save podcast & episodes
   INSERT INTO podcasts (...)
   INSERT INTO podcast_episodes (...)
   â†“
4. Queue episodes for transcription
   For each episode:
   â””â”€ Add to processing queue
   â†“
5. Process episode (background job)
   â”œâ”€ Download audio file
   â”œâ”€ Convert format if needed
   â”œâ”€ Send to Whisper API
   â””â”€ Receive transcript
   â†“
6. Save transcript
   UPDATE podcast_episodes
   SET transcript = ?,
       transcription_status = 'completed'
   â†“
7. Analyze content (optional)
   - Extract key points
   - Identify speakers
   - Generate summary
   â†“
8. Make searchable
   - Chunk transcript
   - Generate embeddings
   - Store in search index
   â†“
9. Notify user
   - Status update via WebSocket
   - Email notification (optional)
```

## ğŸ”„ State Management

### Frontend State Flow

```
Component State
    â†• useState/useReducer
Custom Hooks
    â†• useEffect
API Client
    â†• fetch
Backend API
    â†• routes
Services
    â†• business logic
Database
```

### Real-Time Updates

```
Event Occurs
    â†“
Backend emits socket event
    â†“
Frontend hook listens
    â†“
State updates
    â†“
Component re-renders
    â†“
UI reflects change
```

## ğŸ“Š Performance Optimization

### Caching Strategy

```
Request arrives
    â†“
Check cache
    â”œâ”€ Hit â†’ Return cached data
    â””â”€ Miss â†“
         Query database
             â†“
         Store in cache (TTL: 5 min)
             â†“
         Return data
```

### Batch Processing

```
Multiple embeddings needed
    â†“
Collect into batch (max 20)
    â†“
Single API call
    â†“
Process results
    â†“
Save all at once (transaction)
```

## ğŸ¯ Key Takeaways

1. **Upload**: File â†’ Extract â†’ Chunk â†’ Embed â†’ Store
2. **Search**: Query â†’ (BM25 + Vector) â†’ Fuse â†’ Rank â†’ Return
3. **Transcription**: Audio â†’ Chunks â†’ Whisper â†’ Stream â†’ Save
4. **AI Chat**: Message â†’ Context â†’ AI â†’ Citations â†’ Response
5. **Meeting Bot**: Join â†’ Listen â†’ Transcribe â†’ Respond â†’ Summary
6. **Podcast**: Fetch â†’ Download â†’ Transcribe â†’ Analyze â†’ Search

## ğŸ“š Next Steps

- **[AI Integration](./09_AI_INTEGRATION.md)** - Deep dive into AI services
- **[Frontend](./04_FRONTEND.md)** - UI component implementation
- **[Backend](./05_BACKEND.md)** - API implementation details

---

**Understanding data flow helps you:**
- Debug issues more effectively
- Optimize performance bottlenecks
- Design new features properly
- Make architectural decisions
